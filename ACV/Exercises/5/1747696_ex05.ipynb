{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Amir Hossein Eyvazkhani - 1747696</h4>\n",
        "<h5>Ex 05</h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Task 2</h4>\n",
        "The coverage is checked in Collab. It reaches more than 92 percent accuracy and less than 0.22 loss in the first 20 epochs.\n",
        "In my local machine, I did not have any compute power for showing the output. Sorry about that..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hizFMVF6LW9v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MyBatchNormalization(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        epsilon=1e-5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        param_shape = (input_shape[-1],)\n",
        "        self.gamma = self.add_weight(\n",
        "                name=\"gamma\",\n",
        "                shape=param_shape,\n",
        "                dtype='float32',\n",
        "                trainable=True,\n",
        "            )\n",
        "        self.beta = self.add_weight(\n",
        "                name=\"beta\",\n",
        "                shape=param_shape,\n",
        "                dtype='float32',\n",
        "                trainable=True,\n",
        "        )\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_mean = tf.reduce_mean(inputs, axis=0)\n",
        "        batch_variance = tf.math.reduce_std(inputs, axis=0)\n",
        "        normalized_inputs = (inputs - batch_mean) / tf.sqrt(batch_variance + self.epsilon)\n",
        "        outputs = self.gamma * normalized_inputs + self.beta\n",
        "        return outputs\n",
        "    \n",
        "\n",
        "# for the last part of the questuion\n",
        "class MyBatchNormalization_joint_version(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        epsilon=1e-5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(\n",
        "                name=\"gamma\",\n",
        "                shape=(1,),\n",
        "                dtype='float32',\n",
        "                trainable=True,\n",
        "            )\n",
        "        self.beta = self.add_weight(\n",
        "                name=\"beta\",\n",
        "                shape=(1,),\n",
        "                dtype='float32',\n",
        "                trainable=True,\n",
        "        )\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_mean = tf.reduce_mean(inputs)\n",
        "        batch_variance = tf.math.reduce_std(inputs)\n",
        "        normalized_inputs = (inputs - batch_mean) / tf.sqrt(batch_variance + self.epsilon)\n",
        "        outputs = self.gamma * normalized_inputs + self.beta\n",
        "        return outputs\n",
        "    \n",
        "\n",
        "def define_model():\n",
        "    inputs = keras.Input(shape=(28,28,1))\n",
        "\n",
        "    K = 20 # number of convolution layers per block\n",
        "    L = 3  # number of blocks\n",
        "    x = inputs\n",
        "    for i in range(0,L):\n",
        "        for j in range(0,K):\n",
        "            x = layers.Conv2D(32, 3, activation=\"relu\",padding=\"same\")(x)\n",
        "            x = MyBatchNormalization()(x)\n",
        "            # for the second part we should use this version of BatchNormalization\n",
        "            # x = MyBatchNormalization_joint_version()(x)\n",
        "        x = layers.MaxPooling2D(3)(x)\n",
        "    x = layers.GlobalMaxPooling2D()(x)\n",
        "    outputs = layers.Dense(10,activation='softmax')(x)\n",
        "    model = keras.Model(inputs,outputs)\n",
        "    # model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrNe7WQsOg9s"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess training data (Fashion-MNIST)\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Define and train model\n",
        "model = define_model()\n",
        "model.compile(loss=keras.losses.CategoricalCrossentropy(),optimizer=keras.optimizers.Adam(),metrics=[\"accuracy\"])\n",
        "model.fit(train_images,train_labels, batch_size=64, epochs=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Task 3</h2>\n",
        "<h6></h6>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:From h:\\Uni\\WiSe 2024\\ML LAB\\ml_lab_venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From h:\\Uni\\WiSe 2024\\ML LAB\\ml_lab_venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "938/938 [==============================] - 182s 187ms/step - loss: 0.6447 - accuracy: 0.7724\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 172s 183ms/step - loss: 0.3844 - accuracy: 0.8622\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 180s 192ms/step - loss: 0.3252 - accuracy: 0.8834\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 175s 187ms/step - loss: 0.2957 - accuracy: 0.8949\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 173s 185ms/step - loss: 0.2743 - accuracy: 0.9021\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x204c6bb65d0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def define_model():\n",
        "    # I define the implementation for the model manually here\n",
        "    # in other words, it is not generalized with K, M, and L\n",
        "    # and that is because I could not figure it out when we needed spatial sampling\n",
        "    # also it was mentioned in the question to consider the graph for the implementation\n",
        "    # the name of the layer has the following format: [LAYER_TYPE]_[L]_[K]\n",
        "    # and each M = 2 layer a residual connection is added \n",
        "    inputs = keras.Input(shape=(28,28,1))\n",
        "    conv2d_0_0 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
        "    conv2d_0_1 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(conv2d_0_0)\n",
        "    matchChannel0 = layers.Conv2D(32, 1, padding=\"same\")(inputs)\n",
        "    add0 = layers.add([conv2d_0_1, matchChannel0])\n",
        "    conv2d_0_2 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(add0)\n",
        "    conv2d_0_3 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(conv2d_0_2)\n",
        "    add1 = layers.add([conv2d_0_3, add0])\n",
        "    maxPooling2D_1 = layers.MaxPooling2D(pool_size=(3,3), padding='valid', strides=(1,1))(add1)\n",
        "    conv2d_1_0 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(maxPooling2D_1)\n",
        "    conv2d_1_1 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(conv2d_1_0)\n",
        "    averagePooling2D_1 = layers.MaxPooling2D(pool_size=(3,3), padding='valid', strides=(1,1))(add1)\n",
        "    add2 =  layers.add([conv2d_1_1, averagePooling2D_1])\n",
        "    conv2d_1_2 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(add2)\n",
        "    conv2d_1_3 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(conv2d_1_2)\n",
        "    add3 =  layers.add([conv2d_1_3, add2])\n",
        "    maxPooling2D_2 = layers.MaxPooling2D(pool_size=(3,3), padding='valid', strides=(1,1))(add3)\n",
        "    conv2d_2_0 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(maxPooling2D_2)\n",
        "    conv2d_2_1 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(conv2d_2_0)\n",
        "    averagePooling2D_2 = layers.MaxPooling2D(pool_size=(3,3), padding='valid', strides=(1,1))(add3)\n",
        "    add4 =  layers.add([conv2d_2_1, averagePooling2D_2])\n",
        "    conv2d_2_2 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(add4)\n",
        "    conv2d_2_3 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(conv2d_2_2)\n",
        "    add5 =  layers.add([conv2d_2_3, add4])\n",
        "    falttening = layers.GlobalMaxPooling2D()(add5)\n",
        "    outputs = layers.Dense(10,activation='softmax')(falttening)\n",
        "    model = keras.Model(inputs,outputs)\n",
        "    # model.summary()\n",
        "    return model\n",
        "\n",
        "# Load and preprocess training data (Fashion-MNIST)\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Define and train model\n",
        "model = define_model()\n",
        "model.compile(loss=keras.losses.CategoricalCrossentropy(),optimizer=keras.optimizers.Adam(),metrics=[\"accuracy\"])\n",
        "model.fit(train_images,train_labels, batch_size=64, epochs=5) # just tested with 5 epochs because of the limited computation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It reaches suitable accuracy and loss without batchnorm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BatchNorm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
