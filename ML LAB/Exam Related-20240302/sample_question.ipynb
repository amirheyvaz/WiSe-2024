{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Question\n",
    "\n",
    "The intent of all questions in the exam would be to check your ability to convert algorithm or pseudocode into actual python code. \n",
    "\n",
    "You would not be asked about topics that have not been discussed or covered in PML or ML lectures in the actual exam, however you should still be able to complete this sample exercise with the provided information.\n",
    "\n",
    "The solution is provided at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (15 Points)\n",
    "*You should assume having 40-45 mins to complete this question*\n",
    "\n",
    "\n",
    "### Attention Layer\n",
    "\n",
    "In this part you should implement a simple self-attention layer in Python from scratch without using deep learning libraries like TensorFlow or PyTorch. You are only allowed to use numpy for the matrix operations. We only need to code the feedforward part of the layer.\n",
    "\n",
    "#### Brief Explaination\n",
    "The intuition behind self-attention is giving a neural network the ability to focus on relevant parts of the input data, like a human mind selectively focusing on parts of a given information. In a sequence of words, not all words contribute equally to the meaning of each word in the sentence. Self-attention allows a model to weigh the importance of words relative to each other for a given task, helping it understand context and relationships within the data. It does this by:\n",
    "- Creating Query, Key, and Value vectors for each element in the sequence.\n",
    "- Computing 'attention scores' by matching the Query with each Key, determining the relevance of each element to other elements.\n",
    "- Computing an output for each element by combining the Value vectors, weighted by the attention scores.\n",
    "- In essence, self-attention helps a model to make connections between different elements in a sequence, similar to how humans understand context between different words in a sentence.\n",
    "\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "Each question would be provided with a Pseudocode either in text form as:\n",
    "\n",
    "-------\n",
    "\n",
    "```\n",
    "Define Class SelfAttention:\n",
    "  Define initialization function, __init__()\n",
    "     Input: dims\n",
    "     -  Declare self.dims\n",
    "     -  Initialize weight matrices W_q, W_k, W_v, W_o randomly with shape (dims, dims). The matrices are to be initialized using normal distribution with mean = 0 and std = 1\n",
    "  \n",
    "  Define function forward\n",
    "     Input: x with shape (n, dims)\n",
    "     -  Calculate Q, K, V by performing dot product of x with W_q, W_k and W_v respectively.\n",
    "     -  Calculate dot product of Q and transpose of K. Divide by sqrt of dims.\n",
    "     -  Apply softmax to obtain scores\n",
    "     -  Produce self-attention by calculating dot product of result from step above and V.\n",
    "     -  Produce Output by calculating dot product of result from step above and W_o.\n",
    "     -  Return Output\n",
    "```\n",
    "\n",
    "------\n",
    "\n",
    "***OR* in algorithmic form**\n",
    "\n",
    "Your class should have 2 functions, an `__init__` function\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathrm{Initialize}(dims : num \\ of \\ dimensions \\ for \\ output):\\\\\n",
    "\\\\\n",
    "&W_Q,W_K,W_V,W_O  = Randomly \\ Initalized \\ 2D \\ matrices \\ [dims \\times dims] \\\\ &from \\ Normal \\ Distribution (Mean = 0, Std = 1)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "And a `forward` funciton \n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\mathrm{Foward Pass}(X\\mathrm{: 2D \\ matrix \\ [n \\times dims]}):\\\\\n",
    "\\\\\n",
    "&Q = X \\times W_Q\\\\\n",
    "&K = X \\times W_K,\\\\\n",
    "&V = X \\times W_V\\\\\n",
    "&Score = Q \\times K^T / \\sqrt{dims},\\\\\n",
    "&\\mathrm{Attention \\ Weights} = \\mathrm{softmax}(Score)\\\\\n",
    "&\\mathrm{Self \\ Attention} = \\mathrm{Attention \\ Weights} \\times V\\\\\n",
    "&\\mathrm{Output} = \\mathrm{Self \\ Attention} \\times W_O\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "------\n",
    "\n",
    "Where:\n",
    "- $\\ softmax(x)_i = \\frac{e^{x_{i}}}{\\sum_{j=1}^{n} e^{x_{j}}}\\ $. You need to code this function as well. \n",
    "- The sum of attention weights (i.e. after softmax) along each row should be equal to one. Hence, you need to apply the softmax function row-wise.\n",
    "- \\`$\\times$\\`  denotes matrix multiplication\n",
    "\n",
    "Note: `Score` and `Attention Weights` should of dimension $[n \\times n]$, all other matrices are of shape $[n \\times dims]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(X):\n",
    "    e = np.exp(X)\n",
    "    sum = np.sum(e, axis=1, keepdims=True)\n",
    "    return e/sum\n",
    "\n",
    "class SelfAttention:\n",
    "    def __init__(self, dims):\n",
    "        self.dims = dims\n",
    "        self.W_q = np.random.normal(0.0, 0.1, (dims, dims))\n",
    "        self.W_k = np.random.normal(0.0, 0.1, (dims, dims))\n",
    "        self.W_v = np.random.normal(0.0, 0.1, (dims, dims))\n",
    "        self.W_o = np.random.normal(0.0, 0.1, (dims, dims))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = np.dot(x, self.W_q)\n",
    "        K = np.dot(x, self.W_k)\n",
    "        V = np.dot(x, self.W_v)\n",
    "        score = np.dot(Q, K.T) / np.sqrt(self.dims)\n",
    "        attention_weigths = softmax(score)\n",
    "        self.attention = np.dot(attention_weigths, V)\n",
    "        output = np.dot(self.attention, self.W_o)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_layer = SelfAttention(32)\n",
    "output = my_layer.forward(np.random.random(size=(100,32)))\n",
    "output .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(X):\n",
    "    exp_x = np.exp(X)\n",
    "    return exp_x/exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "\"\"\"\n",
    "Other valid implemenations: \n",
    "def softmax(X):\n",
    "    num =  np.exp(X)\n",
    "    denum = np.exp(X).sum(axis=-1, keepdims=True)\n",
    "    return num/denum\n",
    "\n",
    "def softmax_naive(X):\n",
    "    new_matrix = []\n",
    "    for ii in range(X.shape[0]):\n",
    "        new_row = []\n",
    "        for jj in range(X.shape[1]):\n",
    "            num =  np.exp(X[ii,jj])\n",
    "            new_row.append(num)\n",
    "        new_row = np.array(new_row)\n",
    "        new_row_sum = np.sum(new_row)\n",
    "        new_matrix.append(new_row/new_row_sum)\n",
    "    return np.vstack(new_matrix)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SelfAttention:\n",
    "    def __init__(self, dims):\n",
    "        self.dims = dims    \n",
    "        \n",
    "        self.W_q = np.random.normal(loc=0,scale=1,size=(dims,dims))\n",
    "        self.W_k = np.random.normal(loc=0,scale=1,size=(dims,dims))\n",
    "        self.W_v = np.random.normal(loc=0,scale=1,size=(dims,dims))\n",
    "        self.W_o = np.random.normal(loc=0,scale=1,size=(dims,dims))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # calculation Q,K and V\n",
    "        Q = np.matmul(x, self.W_q)\n",
    "        K = np.matmul(x, self.W_k)\n",
    "        V = np.matmul(x, self.W_v)\n",
    "\n",
    "        score = np.matmul(Q, np.transpose(K))\n",
    "        score = score / np.sqrt(self.dims)\n",
    "        attention_weights =  softmax(score)\n",
    "\n",
    "        self_attention = np.matmul(attention_weights, V)\n",
    "        output = np.matmul(self_attention, self.W_o)\n",
    "\n",
    "        return output\n",
    "    \n",
    "### You can also use A@B notation for matrix multiplication instead of A.matmul(B), they are both equivalent.\n",
    "\n",
    "my_layer = SelfAttention(32)\n",
    "output = my_layer.forward(np.random.random(size=(100,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
