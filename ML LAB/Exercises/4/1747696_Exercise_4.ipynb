{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Machine Learning Lab\n",
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Instructions:**\n",
    "\n",
    "1. You need to submit the PDF as well as the filled notebook file.\n",
    "1. Name your submissions by prefixing your matriculation number to the filename. Example, if your MR is 12345 then rename the files as **\"12345_Exercise_4.xxx\"**\n",
    "1. Complete all your tasks and then do a clean run before generating the final pdf. (_Clear All Ouputs_ and _Run All_ commands in Jupyter notebook)\n",
    "\n",
    "**Exercise Specific instructions::**\n",
    "\n",
    "1. You are allowed to use only NumPy and Pandas (unless stated otherwise). You can use any library for visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization Routines and Loss Functions**\n",
    "\n",
    "In this part of the assignment we learn how to write modular programs and make our code reusable. For this, declare a class named $\\textbf{Optimization}$ which has 2 inputs X and y as the class variables. Next, implement the following optimization algorithms in this class. \n",
    "\n",
    "- Stochastic Gradient Descent (For Mean Square Loss)\n",
    "- Newton’s Method (For Cross Entropy Loss) \n",
    "\n",
    "You will need loss functions and their gradients for the optimization process. So implement a class $\\textbf{Loss}$ which also takes in X and y and computes the following losses and their gradients.\n",
    "\n",
    "- Mean Square Loss (for Regression)\n",
    "- Cross Entropy Loss (for Classification)\n",
    "    \n",
    "Make the $\\textbf{Loss}$ class such that you can access it from the $\\textbf{Optimization}$ class.\n",
    "\n",
    "*Note : You can use np.linalg.solve for solving linear equations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "import numpy as np\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def mean_square_loss(self, theta):\n",
    "        # Mean Square Loss for Regression\n",
    "        predictions = np.dot(self.x, theta)\n",
    "        mse = np.mean((predictions - self.y) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def mean_square_loss_gradient(self, theta):\n",
    "        # Gradient of Mean Square Loss for Regression\n",
    "        gradient = 2 * np.dot(self.x.T, (np.dot(self.x, theta) - self.y))  / len(self.y)\n",
    "        return gradient\n",
    "\n",
    "    def cross_entropy_loss(self, theta):\n",
    "        # Cross Entropy Loss for Classification\n",
    "        m = len(self.y)\n",
    "        h_theta = self.sigmoid(np.dot(self.x, theta))\n",
    "        cost = (-1 / m) * np.sum(self.y * np.log(h_theta) + (1 - self.y) * np.log(1 - h_theta))\n",
    "        return cost\n",
    "\n",
    "    def cross_entropy_loss_gradient(self, theta):\n",
    "        # Gradient of Cross Entropy Loss for Classification\n",
    "        m = len(self.y)\n",
    "        h_theta = self.sigmoid(np.dot(self.x, theta))\n",
    "        gradient = np.dot(self.x.T, (h_theta - self.y)) / m\n",
    "        return gradient\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class Optimization:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.loss = Loss(x, y)\n",
    "\n",
    "    def stochastic_gradient_descent(self, theta, learning_rate, epochs):\n",
    "        # Stochastic Gradient Descent for Mean Square Loss\n",
    "        m = len(self.y)\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(m):\n",
    "                random_index = np.random.randint(m)\n",
    "                xi = self.X[random_index:random_index+1]\n",
    "                yi = self.y[random_index:random_index+1]\n",
    "                gradient = self.loss.mean_square_loss_gradient(theta)\n",
    "                theta = theta - learning_rate * gradient\n",
    "        return theta\n",
    "\n",
    "    def newtons_method(self, theta, epochs):\n",
    "        # Newton's Method for Cross Entropy Loss\n",
    "        for epoch in range(epochs):\n",
    "            cost = self.loss.cross_entropy_loss(theta)\n",
    "            gradient = self.loss.cross_entropy_loss_gradient(theta)\n",
    "            hessian = np.dot(self.x.T, np.dot(np.diag(self.loss.sigmoid(theta)), np.dot(np.diag(1 - self.loss.sigmoid(theta)), self.x)))\n",
    "            theta = theta - np.linalg.solve(hessian, gradient)\n",
    "        return theta\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have X and y as your input data and labels\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([0, 1, 0])\n",
    "theta_init = np.zeros(X.shape[1] + 1)  # Initialize theta with zeros\n",
    "\n",
    "opt = Optimization(X, y)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "theta_sgd = opt.stochastic_gradient_descent(theta_init, learning_rate=0.01, epochs=100)\n",
    "\n",
    "# Newton's Method\n",
    "theta_newton = opt.newtons_method(theta_init, epochs=10)\n",
    "\n",
    "print(\"Theta after Stochastic Gradient Descent:\", theta_sgd)\n",
    "print(\"Theta after Newton's Method:\", theta_newton)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you are given a data set named **\"regression.csv\"**. \n",
    "- Split the dataset into 80% for training and 20% for test\n",
    "- Check the correlation of features (X) with the target (Y) (Visually as well). \n",
    "- Remove the 3 least correlated variables. The correlation is checked only using the train dataset\n",
    "- Perform standard scaling on the remaining feature Columns\n",
    "\n",
    "Implement a class $\\textbf{LinearRegression}$ that has at least two functions, $\\textbf{fit}$ and $\\textbf{predict}$ for fitting a linear regression model and predicting the results. You need to use the $\\textbf{Optimization}$ and $\\textbf{Loss}$ class inside this. Fit a linear regression model with *Mean Square Loss* and *Stochastic Gradient Descent*.\n",
    "\n",
    "Also, generate the loss trajectory for both training and testing datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "\n",
    "Compute the test predictions using the Linear Regression from sklearn and compare the Betas and Results to your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Write your code here\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point to ponder**\n",
    "\n",
    "While optimizing the loss function for Linear Regression or Logistic Regression, one needs to initialize the model parameters. It is well known that deep neural networks do not function if the model parameters are initialized to zero. Why is it so? Does this issue also arise while optimizing the loss function for Linear or Logistic Regression? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a file **\"logistic.csv\"**. \n",
    "- Split the dataset into 80% for training and 20% for test.\n",
    "- Explore the dataset and visualize distribution of the features (train data only). \n",
    "- Do a Violin plot for the 5 features that have the highest standard deviation. \n",
    "- Remove outliers form the dataset. *(This can be done by either removing the rows with outliers or by clipping, comment on the pros and cons of whichever method you employ)*\n",
    "- Perform standard scaling.\n",
    "\n",
    "This part of the assignment involves a classification task. Implement a class $\\textbf{LogisticRegression}$ having at least two functions, $\\textbf{fit}$ and $\\textbf{predict}$ for fitting the model and getting the predictions. Fit a logistic regression model with Cross Entropy Loss and Newton’s Method.\n",
    "\n",
    "Report the test accuracy, plot the confusion matrix and also compute the precision, recall and F-score. \n",
    "\n",
    "Also, generate the loss trajectory for both training and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point to Ponder**\n",
    "\n",
    "Read about precision, recall and F-score. Suppose model A and model B both have same accuracy, but model B has a higher F-score, which model would be suited? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discriminant Analysis**\n",
    "\n",
    "In this part of the assignment you will implement linear and quadratic discriminant analysis classifiers on the iris dataset *from scratch*. Again, this should follow an object oriented method of implementation where you need 2 classes $\\textbf{LDA()}$ and $\\textbf{QDA()}$ with the associated $\\textbf{fit()}$ and $\\textbf{predict()}$ methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#print(iris.DESCR)\n",
    "\n",
    "df_iris = pd.DataFrame(np.hstack([iris.data,iris.target[...,np.newaxis]]),columns=['X1', 'X2', 'X3', 'X4', \"Y\"])\n",
    "\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "\n",
    "Compare your implementation with those of sklearn Library, both in terms of accuracy and timing. Visualize all comparisons in a meaningful manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
