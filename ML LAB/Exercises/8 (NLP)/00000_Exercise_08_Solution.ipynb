{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Machine Learning Lab\n",
    "# Exercise 08\n",
    "\n",
    "**General Instructions:**\n",
    "\n",
    "1. You need to submit the PDF as well as the filled notebook file.\n",
    "1. Name your submissions by prefixing your matriculation number to the filename. Example, if your MR is 12345 then rename the files as **\"12345_Exercise_08.xxx\"**\n",
    "1. Complete all your tasks and then do a clean run before generating the final PDF. (_Clear All Ouputs_ and _Run All_ commands in Jupyter notebook)\n",
    "\n",
    "**Exercise Specific instructions::**\n",
    "\n",
    "1. You are allowed to use only NumPy and Pandas (unless stated otherwise). You can use any library for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import scipy.sparse as sprs\n",
    "from scipy.sparse import lil_matrix\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF and BOW**\n",
    "\n",
    "In this part, you will be working with the IMBD movie review dataset to perform various natural language processing tasks. You need to get the dataset from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "1. Download and read the dataset (subset the data to only use 10,000 rows).\n",
    "1. Perform tokenization on the review text.\n",
    "1. Remove stop words from the tokenized text.\n",
    "1. Use regular expressions to clean the text, removing any HTML tags, emails, and other unnecessary information.\n",
    "1. Convert the cleaned data into a TF-IDF and BOW representation from scratch.\n",
    "\n",
    "*Note: you can use NLTK for all sub-parts except the last*\n",
    "\n",
    "**Main task**:\n",
    "Using the BOW and Tf-Idf representation, implement a Naive-Bayes classifier for the data from scratch. Use Laplace smoothing for the implementation **Do not use sklearn for this part** \n",
    "\n",
    "[Reference Slide](https://www.ismll.uni-hildesheim.de/lehre/ml-16w/script/ml-09-A8-bayesian-networks.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "def preprocess_txt(text):\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "\n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    stop_words = set(stopwords.words('english')) # setting stopwords of English language\n",
    "    words = text.split() # create a list of words\n",
    "    filtered = ' '.join([word for word in words if not word in stop_words]) # removing stopwords\n",
    "    return filtered\n",
    "\n",
    "#cleaning the reviews from html tags, urls, stopwords and punctutation\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df['review'] = df['review'].apply(preprocess_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModels:\n",
    "    def __init__(self, col):\n",
    "        self.col = col #column in which text data is stored\n",
    "        \n",
    "    def fit(self):\n",
    "        self.count_words_across_document = {} #this is used to track the number of comments which contain a particular word\n",
    "        self.list_words_sentences = [] #this list tracks word counts of each comment\n",
    "        for sent in self.col: #iterating over each comment\n",
    "            count_in_sentence = {} #this tracks the word seen in comment so far and its count\n",
    "            for word in sent.split(): #iterating over each word in a comment\n",
    "                \n",
    "                #if word is already in dictionary increase the count else make a new key\n",
    "                if word in self.count_words_across_document:\n",
    "                    self.count_words_across_document[word]+=1\n",
    "                    \n",
    "                elif word not in self.count_words_across_document:\n",
    "                    self.count_words_across_document[word]=1\n",
    "                \n",
    "                #this tracks word in each comment only\n",
    "                if word in count_in_sentence:\n",
    "                    count_in_sentence[word]+=1\n",
    "\n",
    "                elif word not in count_in_sentence:\n",
    "                    count_in_sentence[word]=1\n",
    "            \n",
    "            #make a list containing all the words across all comments\n",
    "            self.list_words_sentences.append(count_in_sentence)\n",
    "            \n",
    "        #to make a two way connection between going from words to index or vice versa\n",
    "        self.idx_to_word= {i:w for i,w in enumerate(self.count_words_across_document)}\n",
    "        self.word_to_idx = {w:i for i,w in enumerate(self.count_words_across_document)}\n",
    "        self.vocab_size = len(self.count_words_across_document) #number of unique words\n",
    "            \n",
    "    def BOW(self):\n",
    "        bow = sprs.lil_matrix((1,self.vocab_size)) #sparse matrix as they are more efficent\n",
    "        for sent in self.list_words_sentences: #iterate over each comment words dictionary\n",
    "            rep = sprs.lil_matrix((1,self.vocab_size)) #empty matrix to store the final representation of the word\n",
    "            for word, count in sent.items(): #iterating ober each word\n",
    "                rep[0,self.word_to_idx[word]]=count #replacing the count in the representation matrix of that word\n",
    "            bow = sprs.vstack([bow,rep]) #concatenate the matrix vertically to obtain one big matrix of size mxd\n",
    "        return bow.toarray()[1:,:] \n",
    "    \n",
    "    def TF_IDF(self):\n",
    "        tokenized_docs,vocab,word_index=self.vocab_index(self.col)\n",
    "        word_counts = Counter([word for doc in tokenized_docs for word in set(doc)])\n",
    "\n",
    "        tf_matrix=lil_matrix((len(tokenized_docs),len(vocab)),dtype=np.float64)\n",
    "\n",
    "        for i,doc in enumerate(tokenized_docs):\n",
    "            for word in doc:\n",
    "                tf_matrix[i,word_index[word]]=doc.count(word)\n",
    "\n",
    "        idf_matrix = lil_matrix((len(vocab),len(vocab)),dtype=np.float64)\n",
    "\n",
    "        for i,word in enumerate(vocab):\n",
    "            idf_matrix[i,i]=np.log((len(tokenized_docs)+1)/word_counts[word])\n",
    "\n",
    "        tf_idf_matrix=tf_matrix*idf_matrix\n",
    "        return tf_idf_matrix.toarray()\n",
    "\n",
    "    def vocab_index(self,X):\n",
    "        tokenized_docs=[preprocess_txt(X.iloc[i]).split() for i in range(self.col.shape[0])]\n",
    "        vocab=set([word for doc in tokenized_docs for word in doc])\n",
    "        word_index = {word :i for i,word in enumerate(vocab)}\n",
    "        return tokenized_docs,vocab,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make the model using only the first 1000 reviews, the code will remain same for the analysis over whole data\n",
    "n = NLPModels(df['review'].iloc[:1000]) \n",
    "#fit the model\n",
    "n.fit()\n",
    "#obtain the BOW representation\n",
    "bow_representation = n.BOW()\n",
    "#obtain TF-IDF representation\n",
    "tfidf_representation = n.TF_IDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(df.iloc[:1000,-1]=='positive',1,0) #to make reviews binary where 1=positive and 0=negative review\n",
    "#first using the bow representation\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_representation,y,random_state=42, test_size=0.2)\n",
    "#let's try with TF-IDF representation now\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_representation,y,random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__ (self, alpha, vocab_size):\n",
    "        self.alpha = alpha #smoothing parameter alpha\n",
    "        self.vocab_size = vocab_size #vocab size of the words\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X #store the features\n",
    "        self.y = y #store the targets\n",
    "        m,n = self.X.shape\n",
    "        \n",
    "        #num classes and counts of unique classes in data\n",
    "        self.classes, counts = np.unique(self.y,return_counts=True)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        #To store conditional probability of all the words\n",
    "        self.cond_prob = np.zeros(shape=(n,n_classes))\n",
    "        \n",
    "        #To store prior probability of all the words \n",
    "        self.prior_prob = [cnt/m for cnt in counts]\n",
    "        \n",
    "        #iterate over each class\n",
    "        for cls in self.classes:\n",
    "            idx = self.y==cls #indices of all instances which belong to particular class\n",
    "            sub_x = self.X[idx,:] #sample the dataset of all observation belonging to particular class\n",
    "            \n",
    "            #calculate the conditional probability using laplacian smoothing\n",
    "            self.cond_prob[:,cls] = (sub_x.sum(axis=0) + self.alpha)/(sub_x.sum() + self.alpha * self.vocab_size)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        preds = [] #to store the predictions\n",
    "        for x in X: #iterative over all test instances\n",
    "            class_prob = [] #store probability for each instance\n",
    "            for cls in self.classes: #iterate over all classes\n",
    "                idx = self.y == cls #store indices of particular class samples\n",
    "                sub_x = self.X[idx,:] #sample the observation belonging to particular class\n",
    "                \n",
    "                #to calculate the probability\n",
    "                likelihood = np.multiply(np.log(self.cond_prob[:,cls].reshape(1,-1)),x.reshape(1,-1))\n",
    "                prob = np.sum(likelihood) + np.log(self.prior_prob[cls])\n",
    "                \n",
    "                #store the probability of belonging to each class\n",
    "                class_prob.append(prob)\n",
    "            \n",
    "            #the class with maximum probability is the class of an instance\n",
    "            preds.append(np.argmax(class_prob))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the model while using BOW on test set is 0.795\n",
      "The Accuracy of the model while using TF-IDF on test set is 0.735\n"
     ]
    }
   ],
   "source": [
    "N = NaiveBayes(alpha=1,vocab_size=n.vocab_size)\n",
    "N.fit(X_train_bow,y_train_bow)\n",
    "print('The Accuracy of the model while using BOW on test set is',accuracy_score(y_test_bow,N.predict(X_test_bow)))\n",
    "\n",
    "N = NaiveBayes(alpha=1,vocab_size=n.vocab_size)\n",
    "N.fit(X_train_tfidf,y_train_tfidf)\n",
    "print('The Accuracy of the model while using TF-IDF on test set is',accuracy_score(y_test_tfidf,N.predict(X_test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "\n",
    "Use sklearn implementation of Naive-Bayes classifier and compare the results with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the Sklearn model while using BOW on test set is 0.795\n",
      "The Accuracy of the Sklearn model while using TF-IDF on test set is 0.735\n"
     ]
    }
   ],
   "source": [
    "### Your code here\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow,y_train_bow)\n",
    "print('The Accuracy of the Sklearn model while using BOW on test set is',accuracy_score(y_test_bow,nb.predict(X_test_bow)))\n",
    "\n",
    "#to compare from sklearn's implementation\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf,y_train_tfidf)\n",
    "print('The Accuracy of the Sklearn model while using TF-IDF on test set is',accuracy_score(y_test_tfidf,nb.predict(X_test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Accuracy of our model using both BOW and TFIDF and the accuracy of Sklearn's implementation are same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-gram Language Model**\n",
    "\n",
    "\n",
    "You won't believe what happened ??? !\n",
    "\n",
    "Is the word \"next\" on the tip of your tongue? Although there are other possibilities, that is undoubtedly the most likely one. Other options are \"after\", \"after that\", and \"to them\". Our intuition tells us that some sentence endings are more plausible than others, especially when we take into account the previous information, the location of the phrase, and the speaker or author.\n",
    "\n",
    "N-gram language models simply formalize that intuition. An n-gram model gives each possibility a probability score by solely taking into account the words that came before it. The probability of the word \"next\" in our example may be 80\\%, whereas the probabilities of the words \"after\" and \"then\" might be 10\\%, 5\\%, and 5\\%, respectively.\n",
    "\n",
    "By leveraging these statistics, n-grams fuel the development of language models, which in turn contribute to an overall speech recognition system.\n",
    "\n",
    "**Main task**:\n",
    "\n",
    "In this part you are tasked with coding a N-gram language model on the dataset (https://www.kaggle.com/datasets/nltkdata/europarl). Use the english language for the task.\n",
    "\n",
    "\n",
    "Evaluate your model based on perplexity and generate sentences using n-grams with n={2,3,4,5}. \n",
    "\n",
    "*Reading Material: https://web.stanford.edu/~jurafsky/slp3/3.pdf*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data first which is extracted in a folder named english\n",
    "folder_name = 'english'\n",
    "corpus = []\n",
    "for fname in os.listdir(folder_name):\n",
    "    fname = os.path.join(folder_name, fname)\n",
    "    with open(fname,'r',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text = line.lower()\n",
    "            filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "            translate_dict = dict((c, \" \") for c in filters)\n",
    "            translate_map = str.maketrans(translate_dict)\n",
    "            text = text.translate(translate_map)\n",
    "            sent = ' '.join(text.strip().split())\n",
    "            if len(sent)>1:\n",
    "                cleaned_sent = '<s> '+sent+' </s>'\n",
    "                corpus.append(cleaned_sent)\n",
    "                \n",
    "#doing a train test split                \n",
    "train_data, test_data = train_test_split(np.array(corpus),test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "class Ngram:\n",
    "    def __init__ (self, n, alpha=1):\n",
    "        self.n = n \n",
    "        self.alpha = alpha\n",
    "        self.initials = ''\n",
    "    \n",
    "    def make_ngram(self, corpus):\n",
    "        self.word_dict = {} #to store key value pairs\n",
    "        self.key_counts = {} #to track count of keys\n",
    "        self.togther_counts = {} #to track how many times they appeared togather\n",
    "        for sent in corpus: #iterate over each sentence\n",
    "            words = sent.split() \n",
    "            l = 0 #left pointer\n",
    "            r = self.n-1 #right pointer\n",
    "            \n",
    "            while r<len(words): #iterate over each word\n",
    "                key = tuple(words[l:r])\n",
    "                val = words[r]\n",
    "                togather = tuple(words[l:r] + [words[r]])\n",
    "                \n",
    "                if not key in self.word_dict:\n",
    "                    self.word_dict[key] = {val:0}\n",
    "                    self.key_counts[key] = 0\n",
    "                    \n",
    "                if not val in self.word_dict[key]:\n",
    "                    self.word_dict[key][val] = 0\n",
    "                   \n",
    "                if not togather in self.togther_counts:\n",
    "                    self.togther_counts[togather] = 0\n",
    "                \n",
    "                self.word_dict[key][val]+=1\n",
    "                self.key_counts[key] +=1\n",
    "                self.togther_counts[togather] +=1\n",
    "                l+=1\n",
    "                r+=1\n",
    "                \n",
    "            if not tuple([val]) in self.key_counts:\n",
    "                self.key_counts[tuple([val])] = 0\n",
    "            self.key_counts[tuple([val])] += 1\n",
    "            self.build_vocab()\n",
    "            \n",
    "    def build_vocab(self):\n",
    "        self.vocab = set()\n",
    "        for k in self.key_counts.keys():\n",
    "            for i in k:\n",
    "                self.vocab.add(i) #this makes the vocabulory\n",
    "    \n",
    "    def generate_next_word(self, initials, c=0, max_sentence_len=100):\n",
    "        #iterate over each word and find its prob but it's wiser to first calculate probs with words that appeared togather\n",
    "        key = tuple(initials)\n",
    "        \n",
    "        #see if we have seen these initials before in corpus\n",
    "        if key in self.word_dict:\n",
    "            words = self.word_dict[key]\n",
    "              \n",
    "        #if not then we will pick some random words and check their probabilities with our initials\n",
    "        else:\n",
    "            idx = np.random.randint(len(self.vocab))\n",
    "            word = list(self.vocab)\n",
    "            words = {word[idx]:0}\n",
    "\n",
    "        P = {}\n",
    "        #see how many times this word appear with our last n words\n",
    "        for word,count in words.items():\n",
    "            if key in self.key_counts:\n",
    "                prob = (count + self.alpha)/(self.key_counts[key]+self.alpha*(len(self.vocab)-1))\n",
    "                P[word] = prob\n",
    "            else:\n",
    "                P[word] = (count + self.alpha)/(0+self.alpha*(len(self.vocab)-1))\n",
    "        \n",
    "        if len(self.initials) >max_sentence_len:\n",
    "            return self.initials\n",
    "        \n",
    "        #pick the most probable word and predict it\n",
    "        words_probabilities = sorted(P.items(), key=lambda x:x[1])\n",
    "        #most probable word\n",
    "        w, _ = words_probabilities[-1]\n",
    "\n",
    "        if c==0:\n",
    "            self.initials=initials\n",
    "\n",
    "        self.initials += [w]\n",
    "\n",
    "        if w == \"</s>\":\n",
    "            return self.initials\n",
    "\n",
    "        self.generate_next_word(self.initials[-self.n+1:],c+1)\n",
    "            \n",
    "    def measure_perplexity(self, sentences):\n",
    "        P = [] #empty list to store probabilites\n",
    "        N=0 #number of words\n",
    "        for sent in sentences: #iterate over each sentence\n",
    "            \n",
    "            l = 0 #left pointer\n",
    "            r = self.n-1 #right pointer\n",
    "            \n",
    "            #iterate till right pointer reaches the end of sentence\n",
    "            while r<len(sent):\n",
    "                N+=1 #number of words\n",
    "                key = tuple(sent[l:r]) #the probability will be conditioned on the previous n words\n",
    "                wi = sent[r] #the words whose probability we have to find\n",
    "\n",
    "                #see if we have seen these initials before in corpus\n",
    "                if key in self.word_dict:\n",
    "                    #if the previous n words have occured togather get their count\n",
    "                    w_ = self.key_counts[key]\n",
    "                    \n",
    "                    #if the word whose probability we have to find is in corpus, get its  count, else set the count to zero\n",
    "                    if wi in self.word_dict[key]:\n",
    "                        count = self.word_dict[key][wi]\n",
    "                    else:\n",
    "                        count = 0\n",
    "\n",
    "                #if neither the initials nor the word has occured togather, we will set both to zero (laplace smoothing will take care)\n",
    "                else:\n",
    "                    count = 0\n",
    "                    w_ = 0\n",
    "\n",
    "                #to get the conditional probability of the word\n",
    "                p=(count + self.alpha)/(w_+self.alpha*(len(self.vocab)-1))\n",
    "                \n",
    "                #to avoid getting smaller values take log\n",
    "                P.append(np.log2(p))\n",
    "                l+=1\n",
    "                r+=1\n",
    "        \n",
    "        #transform it back and return \n",
    "        return 2**(-1/N*np.sum(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate some text by our n-gram model with n=2,3,4,5. For keeping the scale of computations limited we will only train on first 1000 sentences and measure perplexity on them. This will give us good relative measure of the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which generate sentences\n",
    "def generate_sentences(text,n,train_data=train_data,train_size=1000):\n",
    "    ng = Ngram(n,alpha=1)\n",
    "    ng.make_ngram(train_data[:train_size])\n",
    "    initials = text.split()[:n-1]\n",
    "    ng.generate_next_word(initials)\n",
    "    return ' '.join(ng.initials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government </s>\n"
     ]
    }
   ],
   "source": [
    "#Let's take a sample text as \"european president has died\" as a sample text and see what type sentences does our model generate\n",
    "sample_text = 'government has announced elections'\n",
    "\n",
    "#for n=2\n",
    "print(generate_sentences(sample_text,n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'government has expressed its satisfaction with the european union </s>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for n=3\n",
    "generate_sentences(sample_text,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'government has announced november staff desirable confine günter elements attained stressing individuality accommodate vertical institutional clare inaugural securities draft christmas hopes danger tricked disadvantages over varying architect turns constitutes shape mighty applying appreciates striving personally concentrate reconstruction legal pardon match sea guarantee explained he grave started renovate absence speed thing timetable join doubtless resulted birds trouble dragged about clout tankers competitivity demonstrated swimming those growth honour exercised observations arrangement circumstances quote possibility effects peacefully intend injustices sailing reigns times sure managing card padania responsible decentralisation accused doubt employers memories lines labour memory solutions cases derivatives birds china example sections rules radical'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for n=4\n",
    "generate_sentences(sample_text,n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'government has announced elections easier light forthcoming decentralising cooperate voted subjects rapporteur paragraph anyone protocols examine nothing dispose workings liabilities changing stressing management embargo therefore valid discussed 18 prospects dossiers hears being concretely intends margin inherent ahead fire gain shape suffering 35 reservoir whatsoever safety prevention potential fight forgive knörr debating year matters recreational million hopefully gnp regions opened flexible satisfactory dredging reserves compromise 10 dgs widely management quite liability conscience divided denouncing farming substances reliable limitation copyright discussed separate preparatory past foul alarming cent 69 precedence alongside huge simplify extent corruption september toned approved mothers exemption break arrested rights stand'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for n=5\n",
    "generate_sentences(sample_text,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Perplexity of n=2 model is 3931.2996248844865\n"
     ]
    }
   ],
   "source": [
    "#to check perplexity\n",
    "#to keep the size of the computations in control we will calculate perplexity over first 1000 sentences of training set\n",
    "train = train_data[:1000]\n",
    "test = test_data[:1000]\n",
    "\n",
    "#perplexity of n=2 model\n",
    "ng = Ngram(n=2,alpha=1)\n",
    "ng.make_ngram(train)\n",
    "print('The Perplexity of n=2 model is', ng.measure_perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Perplexity of n=3 model is 3899.0071524044215\n"
     ]
    }
   ],
   "source": [
    "#perplexity of n=2 model\n",
    "ng = Ngram(n=3,alpha=1)\n",
    "ng.make_ngram(train)\n",
    "print('The Perplexity of n=3 model is', ng.measure_perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Perplexity of n=4 model is 3899.000000000003\n"
     ]
    }
   ],
   "source": [
    "#perplexity of n=2 model\n",
    "ng = Ngram(n=4,alpha=1)\n",
    "ng.make_ngram(train)\n",
    "print('The Perplexity of n=4 model is', ng.measure_perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Perplexity of n=5 model is 3897.999999999988\n"
     ]
    }
   ],
   "source": [
    "#perplexity of n=2 model\n",
    "ng = Ngram(n=5,alpha=1)\n",
    "ng.make_ngram(train)\n",
    "print('The Perplexity of n=5 model is', ng.measure_perplexity(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can observe the decreasing perplexity here, note that the perplexity is calculated on portion of train and test data, also the data was randomly sampled into training and test set. So a large value of perplexity does not neccessarily mean that it is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
