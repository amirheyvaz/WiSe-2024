{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Machine Learning Lab\n",
    "# Exercise 10\n",
    "\n",
    "**General Instructions:**\n",
    "\n",
    "1. You need to submit the PDF as well as the filled notebook file.\n",
    "1. Name your submissions by prefixing your matriculation number to the filename. Example, if your MR is 12345 then rename the files as **\"12345_Exercise_10.xxx\"**\n",
    "1. Complete all your tasks and then do a clean run before generating the final PDF. (_Clear All Ouputs_ and _Run All_ commands in Jupyter notebook)\n",
    "\n",
    "**Exercise Specific instructions::**\n",
    "\n",
    "1. You are allowed to use only NumPy and Pandas (unless stated otherwise). You can use any library for visualizations.\n",
    "1. Incase you require a GPU for the 2nd Part, try Google Colab or Kaggle and create a separate PDF report for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees**\n",
    "\n",
    "In this part, you need to implement a decision tree for classification. \n",
    "\n",
    "- Implement an object class **\"Decision_Tree\"** with learn and predict methods. The class should work with multiple **Quality-criterion**. (Accuracy, Information Gain, Misclassification Rate (MCR)) \n",
    "- Implement appropriate stopping criterion i.e. max depth, gain is too small, minimum number of samples for splitting. You can have one or more stopping criterias.\n",
    "- Download and read the Nursery dataset. Link: https://archive.ics.uci.edu/ml/datasets/Nursery\n",
    "- Once the data is loaded, split the data into 70-20-10 split for train/validation/test. *(You can use sklearn for splitting the dataset)*\n",
    "- Train your **\"Decision_Tree\"** with different hyperparameters\n",
    "    - Perform either grid or random search. *(You can use sklearn for hyperparameter search)*\n",
    "    - Hyperparameters can include max-depth, minimunm gain for splitting, minimum number of samples for splitting. Quality-criterion must be one of the hyperparameter. \n",
    "    - Compare the results on validation data. \n",
    "    - Report the test results for the best model. \n",
    "    - Print the best tree using a breath first tree traversal (only till depth of 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        self.value = value\n",
    "        \n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, min_samples=None, max_depth=None, minimunm_gain=None, criterion=\"information-gain\"):\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.minimunm_gain = minimunm_gain\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def split_data(self, dataset, feature, threshold):\n",
    "        # Create empty arrays to store the left and right datasets\n",
    "        left_dataset = []\n",
    "        right_dataset = []\n",
    "        \n",
    "        # Loop over each row in the dataset and split based on the given feature and threshold\n",
    "        for row in dataset:\n",
    "            if row[feature] <= threshold:\n",
    "                left_dataset.append(row)\n",
    "            else:\n",
    "                right_dataset.append(row)\n",
    "\n",
    "        # Convert the left and right datasets to numpy arrays and return\n",
    "        left_dataset = np.array(left_dataset)\n",
    "        right_dataset = np.array(right_dataset)\n",
    "        return left_dataset, right_dataset\n",
    "\n",
    "    def entropy(self, y):\n",
    "        entropy = 0\n",
    "\n",
    "        # Find the unique label values in y and loop over each value\n",
    "        labels = np.unique(y)\n",
    "        for label in labels:\n",
    "            # Find the examples in y that have the current label\n",
    "            label_examples = y[y == label]\n",
    "            # Calculate the ratio of the current label in y\n",
    "            pl = len(label_examples) / len(y)\n",
    "            # Calculate the entropy using the current label and ratio\n",
    "            entropy += -pl * np.log2(pl)\n",
    "\n",
    "        # Return the final entropy value\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, parent, left, right):\n",
    "        # set initial information gain to 0\n",
    "        information_gain = 0\n",
    "        # compute entropy for parent\n",
    "        parent_entropy = self.entropy(parent)\n",
    "        # calculate weight for left and right nodes\n",
    "        weight_left = len(left) / len(parent)\n",
    "        weight_right= len(right) / len(parent)\n",
    "        # compute entropy for left and right nodes\n",
    "        entropy_left, entropy_right = self.entropy(left), self.entropy(right)\n",
    "        # calculate weighted entropy \n",
    "        weighted_entropy = weight_left * entropy_left + weight_right * entropy_right\n",
    "        # calculate information gain \n",
    "        information_gain = parent_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def best_split(self, dataset, num_samples, num_features):\n",
    "        # dictionary to store the best split values\n",
    "        best_split = {'gain': -1, 'feature': None, 'threshold': None}\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            #get the feature at the current feature_index\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            #get unique values of that feature\n",
    "            uniques = np.unique(feature_values).sort()\n",
    "            thresholds = []\n",
    "            for i in range(1, len(uniques)):\n",
    "                thresholds.append(np.average(uniques[i], uniques[i-1]))\n",
    "            thresholds = np.array(thresholds)\n",
    "            # loop over all values of the feature\n",
    "            for threshold in thresholds:\n",
    "                # get left and right datasets\n",
    "                left_dataset, right_dataset = self.split_data(dataset, feature_index, threshold)\n",
    "                # check if either datasets is empty\n",
    "                if len(left_dataset) and len(right_dataset):\n",
    "                    # get y values of the parent and left, right nodes\n",
    "                    y, left_y, right_y = dataset[:, -1], left_dataset[:, -1], right_dataset[:, -1]\n",
    "                    information_gain = -1\n",
    "                    # compute information gain based on the y values\n",
    "                    if self.criterion == \"information-gain\":\n",
    "                        information_gain = self.information_gain(y, left_y, right_y)\n",
    "                    # update the best split if conditions are met\n",
    "                    if information_gain > best_split[\"gain\"]:\n",
    "                        best_split[\"feature\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"left_dataset\"] = left_dataset\n",
    "                        best_split[\"right_dataset\"] = right_dataset\n",
    "                        best_split[\"gain\"] = information_gain\n",
    "        return best_split\n",
    "\n",
    "    def calculate_leaf_value(self, y):\n",
    "        y = list(y)\n",
    "        #get the highest present class in the array\n",
    "        most_occuring_value = max(y, key=y.count)\n",
    "        return most_occuring_value\n",
    "    \n",
    "    def build_tree(self, dataset, current_depth=0):\n",
    "        # split the dataset into X, y values\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        n_samples, n_features = X.shape\n",
    "        # keeps spliting until stopping conditions are met\n",
    "        if self.min_samples:\n",
    "            if n_samples < self.min_samples: \n",
    "                leaf_value = self.calculate_leaf_value(y)\n",
    "                return Node(value=leaf_value)\n",
    "        if self.max_depth:\n",
    "            if current_depth > self.max_depth:\n",
    "                leaf_value = self.calculate_leaf_value(y)\n",
    "                return Node(value=leaf_value)\n",
    "        # Get the best split\n",
    "        best_split = self.best_split(dataset, n_samples, n_features)\n",
    "        if self.minimunm_gain:\n",
    "            if best_split[\"gain\"] < self.minimunm_gain:\n",
    "                leaf_value = self.calculate_leaf_value(y)\n",
    "                return Node(value=leaf_value)\n",
    "        elif best_split[\"gain\"] <= 0:\n",
    "            leaf_value = self.calculate_leaf_value(y)\n",
    "            return Node(value=leaf_value)\n",
    "            # continue splitting the left and the right child. Increment current depth\n",
    "        left_node = self.build_tree(best_split[\"left_dataset\"], current_depth + 1)\n",
    "        right_node = self.build_tree(best_split[\"right_dataset\"], current_depth + 1)\n",
    "        # return decision node\n",
    "        return Node(best_split[\"feature\"], best_split[\"threshold\"],\n",
    "                    left_node, right_node, best_split[\"gain\"])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dataset = np.concatenate((X, y), axis=1)  \n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Create an empty list to store the predictions\n",
    "        predictions = []\n",
    "        # For each instance in X, make a prediction by traversing the tree\n",
    "        for x in X:\n",
    "            prediction = self.make_prediction(x, self.root)\n",
    "            # Append the prediction to the list of predictions\n",
    "            predictions.append(prediction)\n",
    "        # Convert the list to a numpy array and return it\n",
    "        np.array(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    def make_prediction(self, x, node):\n",
    "        # if the node has value i.e it's a leaf node extract it's value\n",
    "        if node.value != None: \n",
    "            return node.value\n",
    "        else:\n",
    "            #if it's node a leaf node we'll get it's feature and traverse through the tree accordingly\n",
    "            feature = x[node.feature]\n",
    "            if feature <= node.threshold:\n",
    "                return self.make_prediction(x, node.left)\n",
    "            else:\n",
    "                return self.make_prediction(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parents</th>\n",
       "      <th>has_nurs</th>\n",
       "      <th>form</th>\n",
       "      <th>children</th>\n",
       "      <th>housing</th>\n",
       "      <th>finance</th>\n",
       "      <th>social</th>\n",
       "      <th>health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6463</th>\n",
       "      <td>pretentious</td>\n",
       "      <td>improper</td>\n",
       "      <td>completed</td>\n",
       "      <td>more</td>\n",
       "      <td>critical</td>\n",
       "      <td>convenient</td>\n",
       "      <td>nonprob</td>\n",
       "      <td>priority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7624</th>\n",
       "      <td>pretentious</td>\n",
       "      <td>critical</td>\n",
       "      <td>foster</td>\n",
       "      <td>2</td>\n",
       "      <td>convenient</td>\n",
       "      <td>inconv</td>\n",
       "      <td>nonprob</td>\n",
       "      <td>priority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8430</th>\n",
       "      <td>pretentious</td>\n",
       "      <td>very_crit</td>\n",
       "      <td>foster</td>\n",
       "      <td>1</td>\n",
       "      <td>convenient</td>\n",
       "      <td>convenient</td>\n",
       "      <td>problematic</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9920</th>\n",
       "      <td>great_pret</td>\n",
       "      <td>less_proper</td>\n",
       "      <td>completed</td>\n",
       "      <td>more</td>\n",
       "      <td>critical</td>\n",
       "      <td>convenient</td>\n",
       "      <td>nonprob</td>\n",
       "      <td>not_recom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6597</th>\n",
       "      <td>pretentious</td>\n",
       "      <td>improper</td>\n",
       "      <td>incomplete</td>\n",
       "      <td>3</td>\n",
       "      <td>convenient</td>\n",
       "      <td>inconv</td>\n",
       "      <td>nonprob</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          parents     has_nurs        form children     housing     finance  \\\n",
       "6463  pretentious     improper   completed     more    critical  convenient   \n",
       "7624  pretentious     critical      foster        2  convenient      inconv   \n",
       "8430  pretentious    very_crit      foster        1  convenient  convenient   \n",
       "9920   great_pret  less_proper   completed     more    critical  convenient   \n",
       "6597  pretentious     improper  incomplete        3  convenient      inconv   \n",
       "\n",
       "           social       health  \n",
       "6463      nonprob     priority  \n",
       "7624      nonprob     priority  \n",
       "8430  problematic  recommended  \n",
       "9920      nonprob    not_recom  \n",
       "6597      nonprob  recommended  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch dataset\n",
    "nursery = fetch_ucirepo(id=76)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = nursery.data.features\n",
    "y = nursery.data.targets\n",
    "\n",
    "# Split the data into 70-20-10 split for train/validation/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n",
    "\n",
    "# model = DecisionTree(min_samples=2)\n",
    "# model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP - Word2Vec Model**\n",
    "\n",
    "In this part, we will learn on how neural networks work on language data. We would be implementing a simple *Continuous bag-of-words (CBOW)* model. (You can read more at https://arxiv.org/abs/1301.3781v3)\n",
    "\n",
    "**You can use any deep learning library (for example, pytorch or tensorflow) for this part.**\n",
    "\n",
    "- Create a custom dataloader for Continuous bag-of-words (CBOW) model. This would take the whole text as input and generate samples to train the model. CBOW usualy takes 'n' words before and after a target word and tries to predict the target word. We will use n=2, so the output of the dataloader would be (B,4), (B,1) for X and y respectively (Where B is batch-size).\n",
    "\n",
    "- Creata a Neural Network with folowing specificaitons \n",
    "    - Embedding layer of size 16\n",
    "    - 2 x Linear layer of size 32\n",
    "    - ReLU Activation for hidden layer\n",
    "\n",
    "Train the model for 50 epochs with cross-entropy loss and visualize the final embeddings to understand similarity between the learned word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
