{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Machine Learning Lab\n",
    "# Exercise 9\n",
    "\n",
    "**General Instructions:**\n",
    "\n",
    "1. You need to submit the PDF as well as the filled notebook file.\n",
    "1. Name your submissions by prefixing your matriculation number to the filename. Example, if your MR is 12345 then rename the files as **\"12345_Exercise_9.xxx\"**\n",
    "1. Complete all your tasks and then do a clean run before generating the final PDF. (_Clear All Ouputs_ and _Run All_ commands in Jupyter notebook)\n",
    "\n",
    "**Exercise Specific instructions::**\n",
    "\n",
    "1. You are allowed to use only NumPy and Pandas (unless stated otherwise). You can use any library for visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_digits\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will code a perceptron. It is simply a single node neural network which processes weighted inputs and performs binary classification. \n",
    "\n",
    "\n",
    "- Read up on perceptron algorithm. (https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm_for_a_single-layer_perceptron).\n",
    "- Create an object class caled perceptron. \n",
    "- Train your perceptron using the following different datasets and report the test losses.\n",
    "- Create an animation of how the decision boundary is updated over the iterations. *You can use any library for this viualization*\n",
    "- We will use toy datasets for the problem. Set aside 20% of samples from each dataset for testing.\n",
    "    - **Xlin_sep.npy** and **ylin_sep.npy**. This dataset is linearly separable. Run your algorithm for this data, and you should achieve 100% train and test accuracies!\n",
    "    - **Xlinnoise_sep.npy** and **ylinnoise_sep.npy**. This dataset is not linearly separable and contains noise. Run your algorithm for this data and observe what happens to the decision boundary in the animation. You should get a test accuracy over 80%.\n",
    "    - **circles_x.npy** and **circles_y.npy**. This dataset is non-linear. Devise a strategy to make the dataset separable linearly. *(Hint: Polynomial Features)*. Plot the decision boundary showing how the two classes are separated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funtion for normalizing the features\n",
    "def normalize_features(X,append=True):\n",
    "    X = (X - np.mean(X, 0)) / np.std(X, 0) #normalize the features\n",
    "    if append:\n",
    "        X = np.append(np.ones(X.shape[0]).reshape(-1,1),X,1) #append column of ones for intercept\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__ (self, X, y, iterations=5):\n",
    "        # Initialize Perceptron with input features X, target labels y, and the number of iterations\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1,1)  # Reshape target labels for consistency\n",
    "        self.m, self.n = self.X.shape  # Number of examples (m) and features (n)\n",
    "        self.i = iterations  # Number of max iterations\n",
    "    \n",
    "    def fit(self, lr=0.1, tolerance=1e-3):\n",
    "        # Initialize random weight vector\n",
    "        new_W = np.ones(shape=(self.n+1,1))\n",
    "        # To check if the weight update becomes insignificant\n",
    "        old_W = new_W.copy()\n",
    "        # To keep track of how the weight values change over iterations\n",
    "        w_hist = []\n",
    "        \n",
    "        # Iterating through max iterations\n",
    "        for epoch in range(self.i):\n",
    "            accuracy = 0\n",
    "            iter_count = 0\n",
    "            for ind, x_i in enumerate(self.X):\n",
    "                # Add bias term to input\n",
    "                x_i = np.insert(x_i, 0, 1).reshape(-1,1)\n",
    "                # Apply step activation function to the dot product of input and weights\n",
    "                y_hat = self.activation(np.dot(x_i.T, new_W)) \n",
    "                # Calculate accuracy over the iteration rounds\n",
    "                accuracy += self.accuracy(self.y[ind], y_hat) \n",
    "                # Append iteration count\n",
    "                iter_count += 1\n",
    "                # Update weights using the perceptron learning rule\n",
    "                if (np.squeeze(y_hat) - self.y[ind]) != 0:\n",
    "                    new_W += lr * ((self.y[ind] - y_hat) / 2 * x_i)\n",
    "\n",
    "            # Store the history of the weight vector after each epoch\n",
    "            w_hist.append(old_W)\n",
    "            \n",
    "            # Print the results after every 10 epochs\n",
    "            if epoch % 10 == 0 or epoch == self.i - 1:\n",
    "                print('Epoch {}/{}'.format(epoch, self.i - 1),':', 'Train Accuracy: {:.4f}'.format(accuracy / iter_count)) \n",
    "\n",
    "            # The new weights become old for the next iteration\n",
    "            old_W = new_W\n",
    "            \n",
    "        # Store final weights and history of weights for visualization purposes\n",
    "        self.W = old_W\n",
    "        self.hist = w_hist\n",
    "    \n",
    "    def activation(self, z):\n",
    "        # Step activation function: 1 for z > 0, -1 otherwise\n",
    "        return np.where(z > 0, 1, -1)     \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Add intercept term to input\n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        # Calculate predictions using the learned weights\n",
    "        self.pred = self.activation(np.dot(X, self.W).reshape(1, -1)) \n",
    "        return self.pred\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        # Calculate accuracy by counting the number of correct predictions\n",
    "        accuracy = np.sum(y_true == y_pred)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4 : Train Accuracy: 0.9067\n",
      "Epoch 4/4 : Train Accuracy: 1.0000\n",
      "The accuracy on Train Set is 1.0\n",
      "The accuracy on Test Set is 1.0\n"
     ]
    }
   ],
   "source": [
    "#Linearly Separable Data\n",
    "\n",
    "#for loading the data\n",
    "with open('Xlin_sep.npy','rb') as f:\n",
    "    X = np.load(f)\n",
    "with open('ylin_sep.npy','rb') as f:\n",
    "    y = np.load(f)\n",
    "    \n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#normalize training set\n",
    "X_train_normalized = normalize_features(X_train, append=False)\n",
    "\n",
    "#normalizing test set\n",
    "X_test_normalized = normalize_features(X_test,append=False)\n",
    "\n",
    "#making the model\n",
    "p = Perceptron(X_train_normalized, y_train)\n",
    "#fitting the perceptron\n",
    "p.fit()\n",
    "\n",
    "#to get predictions on train and test set\n",
    "preds_train = p.predict(X_train_normalized)\n",
    "preds_test = p.predict(X_test_normalized)\n",
    "\n",
    "print('The accuracy on Train Set is',p.accuracy(y_train,preds_train)/len(y_train))\n",
    "print('The accuracy on Test Set is',p.accuracy(y_test,preds_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualize:\n",
    "    \n",
    "    def __init__(self, X, y, W):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.W = W\n",
    "    \n",
    "    def create_frame(self, w, iteration, name='Linearly Separable'):\n",
    "    \n",
    "        if not os.path.exists(name):\n",
    "            os.mkdir(name)\n",
    "        markers = ('s', 'o')\n",
    "        colors = ('red', 'blue')\n",
    "        cmap = plt.cm.RdBu\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        # Plot the decision surface\n",
    "        x1_min, x1_max = self.X[:, 0].min() - 1, self.X[:, 0].max() + 1\n",
    "        x2_min, x2_max = self.X[:, 1].min() - 1, self.X[:, 1].max() + 1\n",
    "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01),\n",
    "                               np.arange(x2_min, x2_max, 0.01))\n",
    "        \n",
    "        X = np.array([xx1.ravel(), xx2.ravel()]).T\n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "            \n",
    "        Z = np.dot(X, w).reshape(1, -1)\n",
    "        Z = np.where(Z > 0, 1, -1)\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "        \n",
    "        ax.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "\n",
    "        # Plot all samples\n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            ax.scatter(x=self.X[self.y == cl, 0], y=self.X[self.y == cl, 1],\n",
    "                       alpha=0.8, c=colors[idx],\n",
    "                       marker=markers[idx], label=f'Class {cl}')\n",
    "\n",
    "\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.set_title(f'Decision Boundary of {name} at iteration {iteration}')\n",
    "\n",
    "        ax.legend(loc='upper left')\n",
    "        plt.savefig(f'./{name}/img_{iteration}.png', \n",
    "                        transparent = False,  \n",
    "                        facecolor = 'white'\n",
    "                       )\n",
    "        plt.close()\n",
    "\n",
    "    def create_animation(self,name):\n",
    "\n",
    "        for t,w in enumerate(self.W):\n",
    "            self.create_frame(w,t,name)\n",
    "\n",
    "        frames = []\n",
    "        for t in range(len(self.W)):\n",
    "            image = imageio.v2.imread(f'./{name}/img_{t}.png')\n",
    "            frames.append(image)\n",
    "\n",
    "        imageio.mimsave(f'./{name}.gif', frames, fps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history of weight vectors\n",
    "W = p.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize(X_train_normalized,y_train,W).create_animation('train_lin_sep_')\n",
    "Visualize(X_test_normalized,y_test,W).create_animation('test_lin_sep_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train_lin_sep_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_lin_sep_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data with Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading the data\n",
    "with open('Xlinnoise_sep.npy','rb') as f:\n",
    "    X = np.load(f)\n",
    "with open('ylinnoise_sep.npy','rb') as f:\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=14)\n",
    "\n",
    "#normalize training set\n",
    "X_train_normalized = normalize_features(X_train,append=False)\n",
    "\n",
    "#normalizing test set\n",
    "X_test_normalized = normalize_features(X_test,append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19 : Train Accuracy: 0.7933\n",
      "Epoch 10/19 : Train Accuracy: 0.7667\n",
      "Epoch 19/19 : Train Accuracy: 0.7667\n"
     ]
    }
   ],
   "source": [
    "#making the model\n",
    "p = Perceptron(X_train_normalized,y_train,iterations=20)\n",
    "#fitting the perceptron\n",
    "p.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get predictions on train and test set\n",
    "preds_train = p.predict(X_train_normalized)\n",
    "preds_test = p.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on Train Set is 0.82\n",
      "The accuracy on Test Set is 0.78\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy on Train Set is',p.accuracy(y_train,preds_train)/len(y_train))\n",
    "print('The accuracy on Test Set is',p.accuracy(y_test,preds_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history of weight vectors\n",
    "W = p.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize(X_train_normalized,y_train,W).create_animation('train_linnoise_sep_')\n",
    "Visualize(X_test_normalized,y_test,W).create_animation('test_linnoise_sep_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train_linnoise_sep_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_linnoise_sep_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Circle Data (Non-Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading the data\n",
    "with open('circles_x.npy','rb') as f:\n",
    "    X = np.load(f)\n",
    "with open('circles_y.npy','rb') as f:\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is not linearly separable, we will add polynomial features into the data. This means instead of giving the perceptron x1 and x2 as input we will now give it  $ x_1^2, x_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.square(X), y, test_size=0.25, random_state=42)\n",
    "\n",
    "#normalize training set\n",
    "X_train_normalized = normalize_features(X_train,append=False)\n",
    "\n",
    "#normalizing test set\n",
    "X_test_normalized = normalize_features(X_test,append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19 : Train Accuracy: 0.7933\n",
      "Epoch 10/19 : Train Accuracy: 1.0000\n",
      "Epoch 19/19 : Train Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#making the model\n",
    "p = Perceptron(X_train_normalized,y_train,iterations=20)\n",
    "#fitting the perceptron\n",
    "p.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get predictions on train and test set\n",
    "preds_train = p.predict(X_train_normalized)\n",
    "preds_test = p.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on Train Set is 1.0\n",
      "The accuracy on Test Set is 1.0\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy on Train Set is',p.accuracy(y_train,preds_train)/len(y_train))\n",
    "print('The accuracy on Test Set is',p.accuracy(y_test,preds_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history of weight vectors\n",
    "W = p.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize(X_train_normalized,y_train,W).create_animation('train_circle_')\n",
    "Visualize(X_test_normalized,y_test,W).create_animation('test_circle_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train_circle_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_circle_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will create a feed-forward neural network\n",
    "\n",
    "- Load the MNIST classification dataset using sklearn. Split the data into train and test datasets (80-20 split).\n",
    "- Implement a neural network with forward propagation and backpropagation **from scratch**.\n",
    "- Use Stochastic Gradient Descent as the optimizer and Cross-entropy as Loss.\n",
    "- You model class should be flexible in terms of\n",
    "    - Number of layers\n",
    "    - Number of hidder parameters.\n",
    "    - Activation function for each layer (SoftMax, ReLU or tanh)\n",
    "- Now create a training function that takes the neural network and training data as inputs and updates the weights of the network. This function should also take in the learning rate, number of epochs, and batchsize as input.\n",
    "- Try out different hyperparameters to train your model and try to achieve >90% test accuracy. \n",
    "\n",
    "*Hints:*\n",
    "- Flatten the MNIST data from 2D to 1D.\n",
    "- Use *He weights initialization* for weights. *The He initialization calculates the starting weights as randomly generated matrices using a Gaussian probability distribution with a mean of 0.0 and a standard deviation of sqrt(2/n), where n is the number of inputs to the layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Cross-entropy gradient, the gradient of the loss with respect to the output\n",
    "def cross_entropy_gradient(y_pred, y_true):\n",
    "    n_samples = y_true.shape[0]\n",
    "    res = y_pred - y_true\n",
    "    return res / n_samples\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden, hidden_dims, activation_funcs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_hidden = n_hidden  # hidden layers\n",
    "        self.hidden_dims = hidden_dims  # neurons in each hidden layer\n",
    "\n",
    "        # list for the number of neurons in each layer\n",
    "        layers = [self.n_inputs] + self.hidden_dims + [self.n_outputs]\n",
    "        # weights of each layer\n",
    "        self.weights = []\n",
    "        # bias for each layer\n",
    "        self.biases = []\n",
    "        # activation func for each layer\n",
    "        self.activations = []\n",
    "        # derivatives of the weights\n",
    "        self.dW = []\n",
    "        # derivatives of the biases\n",
    "        self.db = []\n",
    "\n",
    "        for i in range(self.n_hidden + 1):\n",
    "            # weights for each layer with \"He\" initialization\n",
    "            self.weights.append(np.random.randn(layers[i], layers[i+1]) * np.sqrt(2. / layers[i]))\n",
    "\n",
    "            # biases for each layer with zeros\n",
    "            self.biases.append(np.zeros((1, layers[i+1])))\n",
    "\n",
    "            # activation function for each layer\n",
    "            self.activations.append(activation_funcs[i])\n",
    "\n",
    "            # derivatives of the weights and biases with zeros\n",
    "            self.dW.append(np.zeros((layers[i], layers[i+1])))\n",
    "            self.db.append(np.zeros((1, layers[i+1])))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # weighted sums for each layer\n",
    "        self.A = []\n",
    "        # activations for each layer\n",
    "        self.Z = []\n",
    "\n",
    "        # For each layer\n",
    "        for i in range(self.n_hidden + 1):\n",
    "            # weighted sum of the inputs\n",
    "            if i == 0:\n",
    "                self.A.append(np.dot(X, self.weights[i]) + self.biases[i])\n",
    "            else:\n",
    "                self.A.append(np.dot(self.Z[i-1], self.weights[i]) + self.biases[i])\n",
    "\n",
    "            # Apply the activation function\n",
    "            if self.activations[i] == 'relu':\n",
    "                self.Z.append(relu(self.A[i]))\n",
    "            elif self.activations[i] == 'softmax':\n",
    "                self.Z.append(softmax(self.A[i]))\n",
    "            elif self.activations[i] == 'tanh':\n",
    "                self.Z.append(tanh(self.A[i]))\n",
    "\n",
    "        # output of the last layer\n",
    "        return self.Z[-1]\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # gradient of the loss with respect to the output\n",
    "        # Note: it's also the gradient for softmax activation layer\n",
    "        # which is why it's not in the loop below\n",
    "        self.dZ = cross_entropy_gradient(output, y)\n",
    "\n",
    "        # gradient of the loss with respect to the weight and bias of the last layer\n",
    "        self.dW[-1] = np.dot(self.Z[-2].T, self.dZ)\n",
    "        self.db[-1] = np.sum(self.dZ, axis=0, keepdims=True)\n",
    "\n",
    "        # For each layer in reverse order\n",
    "        for i in range(self.n_hidden-1, -1, -1):\n",
    "            # gradient of the loss with respect to the weighted sum\n",
    "            if self.activations[i] == 'relu':\n",
    "                self.dZ = np.dot(self.dZ, self.weights[i+1].T) * relu_derivative(self.A[i])\n",
    "            elif self.activations[i] == 'tanh':\n",
    "                self.dZ = np.dot(self.dZ, self.weights[i+1].T) * tanh_derivative(self.A[i])\n",
    "\n",
    "            # gradient of the loss with respect to the weights and biases\n",
    "            if i > 0:\n",
    "                self.dW[i-1] = np.dot(self.Z[i-1].T, self.dZ)\n",
    "            else:\n",
    "                self.dW[i] = np.dot(X.T, self.dZ)\n",
    "            self.db[i] = np.sum(self.dZ, axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, n_epochs, learning_rate, batch_size):\n",
    "        # num of batches based on batch size\n",
    "        n_batches = X.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # for each batch\n",
    "            for batch in range(n_batches):\n",
    "                # batch from data\n",
    "                X_batch = X[batch * batch_size:(batch + 1) * batch_size]\n",
    "                y_batch = y[batch * batch_size:(batch + 1) * batch_size]\n",
    "\n",
    "                # Forward propagation to get output layer\n",
    "                output = self.forward(X_batch)\n",
    "\n",
    "                # Backward propagation to get gradients of the loss function\n",
    "                self.backward(X_batch, y_batch, output)\n",
    "\n",
    "                # Update weights and biases using gradients and learning rate\n",
    "                for i in range(self.n_hidden + 1):\n",
    "                    self.weights[i] -= learning_rate * self.dW[i]\n",
    "                    self.biases[i] -= learning_rate * self.db[i]\n",
    "\n",
    "            # Print loss and accuracy every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                predictions = self.predict(X)\n",
    "                loss = self.calculate_loss(predictions, y)\n",
    "                accuracy = self.calculate_accuracy(predictions, y)\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}, Accuracy = {accuracy}\")\n",
    "\n",
    "    def calculate_loss(self, predictions, y):\n",
    "        # If y is not one-hot encoded, convert it to one-hot encoding\n",
    "        if len(y.shape) == 1:\n",
    "            y_one_hot = np.zeros_like(predictions)\n",
    "            y_one_hot[np.arange(len(y)), y] = 1\n",
    "        else:\n",
    "            y_one_hot = y\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        epsilon = 1e-15\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(y_one_hot * np.log(predictions)) / len(y)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, predictions, y):\n",
    "        # If y is not one-hot encoded, convert it to one-hot encoding\n",
    "        if len(y.shape) == 1:\n",
    "            y_one_hot = np.zeros_like(predictions)\n",
    "            y_one_hot[np.arange(len(y)), y] = 1\n",
    "        else:\n",
    "            y_one_hot = y\n",
    "\n",
    "        # Accuracy calculation\n",
    "        correct_predictions = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_one_hot, axis=1))\n",
    "        accuracy = correct_predictions / len(y)\n",
    "        return accuracy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset mini version, data already flatterned\n",
    "digits = load_digits()\n",
    "#to separate in the features and labels\n",
    "X = digits['data']\n",
    "y = digits['target']\n",
    "#scalar for scaling the features aka pixels\n",
    "scalar = StandardScaler()\n",
    "X = scalar.fit_transform(X) \n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#train valid split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# One-hot encode the labels, 10 posible lables (0 to 9)\n",
    "one_hot = OneHotEncoder(sparse_output=False)\n",
    "y = one_hot.fit_transform(np.array(y).reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.166656993990253, Accuracy = 0.2199025748086291\n",
      "Epoch 10: Loss = 0.6389826570078291, Accuracy = 0.8531663187195546\n",
      "Epoch 20: Loss = 0.3265398462773961, Accuracy = 0.9248434237995825\n",
      "Epoch 30: Loss = 0.2108951207217554, Accuracy = 0.9491997216423104\n",
      "Epoch 40: Loss = 0.15303274940553013, Accuracy = 0.9659011830201809\n",
      "Test accuracy: 94.44%\n"
     ]
    }
   ],
   "source": [
    "# 2 layers to capture complex data relu is most popular layer in all nn's and softmax for classification\n",
    "nn = NeuralNetwork(n_inputs=64, n_outputs=10, n_hidden=2, hidden_dims=[32, 32], activation_funcs=['relu', 'relu', 'softmax'])\n",
    "nn.train(X_train, y_train, n_epochs=50, learning_rate=0.01, batch_size=32)\n",
    "\n",
    "# test accuracy\n",
    "y_pred = np.argmax(nn.predict(X_test), axis=1)\n",
    "\n",
    "# convert encoded y_test to class labels\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "# mean of the comparison between predicted and true labels\n",
    "test_accuracy = np.mean(y_test_labels  == y_pred)\n",
    "print(f'Test accuracy: {test_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLPClassifier**\n",
    "\n",
    "In this part, we will use the same dataset from Part 2 and implement a multi-layer perceptron using sklearn. \n",
    "- Import the necessary classes and perform a 5-fold cross-validation by defining a hyperparameter grid for the MLP classifier. \n",
    "- You need to read about the hyperparameters supported by the function and define a grid for them.\n",
    "- Perform a random search on the grid that you have chosen.\n",
    "- Report a single test accuracy with the best found hyperparameters\n",
    "\n",
    "**Note: you can use any sklearn function for this and the next part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "sklearn_mlp = MLPClassifier(max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=MLPClassifier(max_iter=500), n_jobs=-1,\n",
       "                   param_distributions={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                        &#x27;alpha&#x27;: [0.001, 0.001, 0.01, 0.1],\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        &#x27;solver&#x27;: [&#x27;sgd&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=MLPClassifier(max_iter=500), n_jobs=-1,\n",
       "                   param_distributions={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                        &#x27;alpha&#x27;: [0.001, 0.001, 0.01, 0.1],\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        &#x27;solver&#x27;: [&#x27;sgd&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=500)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=500)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=MLPClassifier(max_iter=500), n_jobs=-1,\n",
       "                   param_distributions={'activation': ['tanh', 'relu'],\n",
       "                                        'alpha': [0.001, 0.001, 0.01, 0.1],\n",
       "                                        'hidden_layer_sizes': [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        'solver': ['sgd']})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Grid\n",
    "grid = {\n",
    "    'hidden_layer_sizes': [(50,30,20),(50,25,10),(25,10)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [0.001, 0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "#fit the grid\n",
    "clf = RandomizedSearchCV(sklearn_mlp, grid, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'sgd',\n",
       " 'hidden_layer_sizes': (50, 30, 20),\n",
       " 'alpha': 0.001,\n",
       " 'activation': 'relu'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "The Neural Network with 3 layers with 50,30, and 20 neurons in each with activation function relu, trained with SGD with Learning Rate 0.001, gave the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy on Test Data is 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "print('The Accuracy on Test Data is',accuracy_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLPRegressor**\n",
    "\n",
    "In this part, we would repeat the all steps taken for MLPClassifier. However, we will try to learn a regression model using MLPRegressor instead. In the end calculate the accuracy of MLPRegressor by using the *test_accuracy_regressor* function provided.\n",
    "\n",
    "**Note: The target output needs to be numerical in this case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=MLPRegressor(max_iter=500), n_jobs=-1,\n",
       "                   param_distributions={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                        &#x27;alpha&#x27;: [0.001, 0.001, 0.01, 0.1],\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        &#x27;solver&#x27;: [&#x27;sgd&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=MLPRegressor(max_iter=500), n_jobs=-1,\n",
       "                   param_distributions={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                        &#x27;alpha&#x27;: [0.001, 0.001, 0.01, 0.1],\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        &#x27;solver&#x27;: [&#x27;sgd&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(max_iter=500)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(max_iter=500)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=MLPRegressor(max_iter=500), n_jobs=-1,\n",
       "                   param_distributions={'activation': ['tanh', 'relu'],\n",
       "                                        'alpha': [0.001, 0.001, 0.01, 0.1],\n",
       "                                        'hidden_layer_sizes': [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        'solver': ['sgd']})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Write your code here\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "sklearn_mlp = MLPRegressor(max_iter=500)\n",
    "\n",
    "#Creating Grid\n",
    "grid = {\n",
    "    'hidden_layer_sizes': [(50,30,20),(50,25,10),(25,10)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [0.001, 0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "#fit the grid\n",
    "reg = RandomizedSearchCV(sklearn_mlp, grid, n_jobs=-1, cv=5)\n",
    "reg.fit(X_train, np.argmax(y_train, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'sgd',\n",
       " 'hidden_layer_sizes': (50, 30, 20),\n",
       " 'alpha': 0.01,\n",
       " 'activation': 'tanh'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "The Neural Network with 3 layers with 50, 30, and 20 neurons in each with activation function tanh, trained with SGD with Learning Rate 0.01, gave the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy on Test Data is 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy_regressor(y_true,y_pred):\n",
    "    ### the function assumes both inputs to be 1-D arrays\n",
    "    assert y_true.shape==y_pred.shape, f\"y_true and y_pred needs to be of same shape, but found y_true: {y_true.shape} and y_pred:{y_pred.shape}\"\n",
    "    assert len(y_pred.shape)==1, f'inputs should be 1-D, but found them as {len(y_pred.shape)}-D'\n",
    "\n",
    "    return np.sum((np.round(y_pred,0).astype(int))==y_true)/y_pred.size\n",
    "\n",
    "test_acc = test_accuracy_regressor(np.argmax(y_test, axis=1),reg.predict(X_test))\n",
    "print('The Accuracy on Test Data is',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the performance of MLP Regressor vs MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier model gave the better accuracy because regression model treated the categorical target and numerical target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
