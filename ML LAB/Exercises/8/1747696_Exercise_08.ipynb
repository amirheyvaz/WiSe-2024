{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Machine Learning Lab\n",
    "# Exercise 08\n",
    "\n",
    "**General Instructions:**\n",
    "\n",
    "1. You need to submit the PDF as well as the filled notebook file.\n",
    "1. Name your submissions by prefixing your matriculation number to the filename. Example, if your MR is 12345 then rename the files as **\"12345_Exercise_11.xxx\"**\n",
    "1. Complete all your tasks and then do a clean run before generating the final PDF. (_Clear All Ouputs_ and _Run All_ commands in Jupyter notebook)\n",
    "\n",
    "**Exercise Specific instructions::**\n",
    "\n",
    "1. You are allowed to use only NumPy and Pandas (unless stated otherwise). You can use any library for visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF and BOW**\n",
    "\n",
    "In this part, you will be working with the IMBD movie review dataset to perform various natural language processing tasks. You need to get the dataset from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "1. Download and read the dataset (subset the data to only use 10,000 rows).\n",
    "1. Perform tokenization on the review text.\n",
    "1. Remove stop words from the tokenized text.\n",
    "1. Use regular expressions to clean the text, removing any HTML tags, emails, and other unnecessary information.\n",
    "1. Convert the cleaned data into a TF-IDF and BOW representation from scratch.\n",
    "\n",
    "*Note: you can use NLTK for all sub-parts except the last*\n",
    "\n",
    "**Main task**:\n",
    "Using the BOW and Tf-Idf representation, implement a Naive-Bayes classifier for the data from scratch. Use Laplace smoothing for the implementation **Do not use sklearn for this part** \n",
    "\n",
    "[Reference Slide](https://www.ismll.uni-hildesheim.de/lehre/ml-16w/script/ml-09-A8-bayesian-networks.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the dataset from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "# Load the dataset and subset it to 10,000 rows\n",
    "df = pd.read_csv('IMDB Dataset.csv', header=0, index_col=None)\n",
    "df = df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Clean text using regular expressions\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # Remove other unnecessary information\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "df['tokenized_text'] = df['cleaned_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['filtered_text'] = df['tokenized_text'].apply(lambda x: [word for word in x if word.isalpha() and word not in stop_words])\n",
    "\n",
    "# Prepare data for classification\n",
    "train_data = df['filtered_text'].to_numpy()[:8000]\n",
    "test_data = df['filtered_text'].to_numpy()[8000:]\n",
    "train_target = df['sentiment'].to_numpy()[:8000]\n",
    "test_target = df['sentiment'].to_numpy()[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.834\n"
     ]
    }
   ],
   "source": [
    "# Naive-Bayes classifier using BOW representation with Laplace smoothing\n",
    "def train_naive_bayes_bow(data, target):\n",
    "    vocabulary = set([word for sublist in data for word in sublist])\n",
    "    word_counts = defaultdict(int)\n",
    "    class_counts = defaultdict(int)\n",
    "    total_docs = len(data)\n",
    "    \n",
    "    for i in range(total_docs):\n",
    "        current_class = target[i]\n",
    "        class_counts[current_class] += 1\n",
    "        \n",
    "        for word in data[i]:\n",
    "            word_counts[(word, current_class)] += 1\n",
    "    \n",
    "    return vocabulary, word_counts, class_counts, total_docs\n",
    "\n",
    "def predict_naive_bayes_bow(vocabulary, word_counts, class_counts, total_docs, document, alpha=1):\n",
    "    scores = defaultdict(float)\n",
    "    \n",
    "    # c is the class label in this iteration\n",
    "    for c in class_counts:\n",
    "        scores[c] = (class_counts[c] / total_docs)\n",
    "        for word in document:\n",
    "            scores[c] *= ((word_counts[(word, c)] + alpha) / (class_counts[c] + alpha * len(vocabulary))) * 1000\n",
    "    # # Normalize scores array\n",
    "    total_score = sum(scores.values())\n",
    "    normalized_scores = {label: score / total_score for label, score in scores.items()}\n",
    "    \n",
    "    # Get the class with the highest probability\n",
    "    predicted_class = max(normalized_scores, key=normalized_scores.get)\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# Train the Naive-Bayes classifier with BOW representation\n",
    "vocabulary, word_counts_per_class, class_counts, total_docs = train_naive_bayes_bow(train_data, train_target)\n",
    "\n",
    "# Test the Naive-Bayes classifier on the test set\n",
    "correct_predictions = 0\n",
    "for i in range(len(test_data)):\n",
    "    prediction = predict_naive_bayes_bow(vocabulary, word_counts_per_class, class_counts, total_docs, test_data[i])\n",
    "    if prediction == test_target[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(test_data)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate TF-IDF representation from scratch\n",
    "def calculate_tfidf(data):\n",
    "    unique_words = set(word for review in data for word in review)\n",
    "    word_count_in_each_doc = {word: np.zeros(len(data)) for word in unique_words}\n",
    "\n",
    "    for i, review in enumerate(data):\n",
    "        for word in review:\n",
    "            word_count_in_each_doc[word][i] += 1\n",
    "\n",
    "    idf = {word: np.log(len(data) / sum(1 for review in data if word in review)) for word in unique_words}\n",
    "\n",
    "    tfidf = {word: [tf_value * idf[word] for tf_value in word_count_in_each_doc[word]] for word in unique_words}\n",
    "\n",
    "    return tfidf\n",
    "\n",
    "# Calculate TF-IDF representation for training and testing data\n",
    "tfidf_train = calculate_tfidf(train_data)\n",
    "print(tfidf_train)\n",
    "# tfidf_test = calculate_tfidf(test_data)\n",
    "\n",
    "# # Function to implement Naive-Bayes classifier with Laplace smoothing\n",
    "# def naive_bayes_classifier(tfidf_data, labels, alpha=1):\n",
    "#     classes = set(labels)\n",
    "#     class_probabilities = {c: sum(1 for label in labels if label == c) / len(labels) for c in classes}\n",
    "\n",
    "#     word_probabilities = {word: {c: [] for c in classes} for word in tfidf_data.keys()}\n",
    "\n",
    "#     for word, values in tfidf_data.items():\n",
    "#         for c in classes:\n",
    "#             word_probabilities[word][c] = (\n",
    "#                 (sum(1 for i, label in enumerate(labels) if label == c and values[i] > 0) + alpha) /\n",
    "#                 (sum(1 for i, label in enumerate(labels) if label == c) + alpha * len(tfidf_data))\n",
    "#             )\n",
    "\n",
    "#     return class_probabilities, word_probabilities\n",
    "\n",
    "# # Train the Naive-Bayes classifier\n",
    "# class_probabilities, word_probabilities = naive_bayes_classifier(tfidf_train, train_labels)\n",
    "\n",
    "# # Function to predict labels using the trained classifier\n",
    "# def predict(tfidf_data, class_probabilities, word_probabilities):\n",
    "#     predictions = []\n",
    "#     classes = class_probabilities.keys()\n",
    "\n",
    "#     for values in zip(*tfidf_data.values()):\n",
    "#         scores = {c: np.log(class_probabilities[c]) for c in classes}\n",
    "\n",
    "#         for word, value in zip(tfidf_data.keys(), values):\n",
    "#             for c in classes:\n",
    "#                 scores[c] += np.log(word_probabilities[word][c] if value > 0 else 1 - word_probabilities[word][c])\n",
    "\n",
    "#         predictions.append(max(scores, key=scores.get))\n",
    "\n",
    "#     return predictions\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# predictions = predict(tfidf_test, class_probabilities, word_probabilities)\n",
    "\n",
    "# # Evaluate the accuracy of the classifier\n",
    "# accuracy = sum(1 for pred, true in zip(predictions, test_labels) if pred == true) / len(test_labels)\n",
    "# print(f'Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "\n",
    "Use sklearn implementation of Naive-Bayes classifier and compare the results with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-gram Language Model**\n",
    "\n",
    "\n",
    "You won't believe what happened ??? !\n",
    "\n",
    "Is the word \"next\" on the tip of your tongue? Although there are other possibilities, that is undoubtedly the most likely one. Other options are \"after\", \"after that\", and \"to them\". Our intuition tells us that some sentence endings are more plausible than others, especially when we take into account the previous information, the location of the phrase, and the speaker or author.\n",
    "\n",
    "N-gram language models simply formalize that intuition. An n-gram model gives each possibility a probability score by solely taking into account the words that came before it. The probability of the word \"next\" in our example may be 80\\%, whereas the probabilities of the words \"after\" and \"then\" might be 10\\%, 5\\%, and 5\\%, respectively.\n",
    "\n",
    "By leveraging these statistics, n-grams fuel the development of language models, which in turn contribute to an overall speech recognition system.\n",
    "\n",
    "**Main task**:\n",
    "\n",
    "In this part you are tasked with coding a N-gram language model on the dataset (https://www.kaggle.com/datasets/nltkdata/europarl). Use the english language for the task.\n",
    "\n",
    "\n",
    "Evaluate your model based on perplexity and generate sentences using n-grams with n={2,3,4,5}. \n",
    "\n",
    "*Reading Material: https://web.stanford.edu/~jurafsky/slp3/3.pdf*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
